---
title: "FINAL PROJECT"
author: "Kevin Phan"
date: '2018-12-04'
output: html_document
---

```{r}
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(usdm)
library(corrplot)
library(Hmisc)
library(FSelector)
library(ROSE)
library(caret)
library(pROC)
library(class) 
library(descr)
library(nortest)
library(magrittr)
library(e1071)
library(caretEnsemble)
library(randomForest)

setwd("/Users/kevinphan/Desktop/CAPSTONE/Capstone")
DF1 <- read.csv("DSI_kickstarterscrape_dataset.csv")
DF2 <- read.csv("MasterKickstarter.csv")
DF3 <- read.csv("ks-projects-201612.csv")
DF4 <- read.csv("18k_Projects.csv")

```


```{r}
#DATA CLEANSING***
DF3 <- DF3[,c(1:8)]
master_data <- join(DF1,DF2, by = "ID", type = "left")
master_data <- join(master_data,DF3, by = "ID", type = "left")
master_data <- master_data[complete.cases(master_data),]
master_data <- master_data[,unique(names(master_data))]
duplicated(colnames(master_data))
str(master_data) #check for variable types. Change where meeded. 
master_data$Deadline <- as.Date(as.character(master_data$Deadline),"%Y-%m-%d")
master_data$Launched <- substr(master_data$Launched,1,10)
master_data$Launched <- as.Date(as.character(master_data$Launched),"%Y-%m-%d")
master_data$Created_At <- as.Date(as.character(master_data$Created_At),"%Y-%m-%d")
master_data$Name <- as.character(master_data$Name)
master_data <- master_data[master_data$Status != "live",]
master_data <- master_data[master_data$Status != "canceled",]
master_data$Staff_Pick[FALSE] = 0
master_data$Staff_Pick <- as.factor(master_data$Staff_Pick)
master_data$spotLight[FALSE] = 0
master_data$spotLight <- as.factor(master_data$spotLight)
master_data$Status <- factor(master_data$Status)#We will not deal with live and cancelled in this analysis.
master_data$Usd_Pledged <- as.numeric(master_data$Usd_Pledged)
master_data$X <- NULL
```



```{r}
#Creating a vector with just the numeric attributes column numbers. This way we can easily reference for them when using alogrithms that can only take numeric attirubtes such as PCA and mahalanobis. 
numericAtt <- vector("numeric",10L)
for (i in 1:ncol(master_data)) {
  if (class(master_data[,i]) == "numeric" || class(master_data[,i]) == "integer" ){numericAtt[i] = i}
  else {numericAtt[i] = NA}
}
numericAtt <- numericAtt[!is.na(numericAtt)][-1]
```

```{r}
#We will now visualize imbalanaces in Status and see Category Frequencies
c = c(unique(master_data$Status))
ggplot(data.frame(master_data$Status),aes(x=master_data$Status),) + geom_bar(fill = c) + labs(title="Frequency of Status",  y="Count", x = "Project Status") 
ggplot(data.frame(master_data$Main.Category),aes(x=master_data$Main.Category)) + geom_bar()
#We see there is way more successful then failed. and category of music is alot more than others. We will fix this in the next step
#There are few major outliers that we can afford to remove. We will use mahalanobis distance.Multivariate
```


```{r}
#Remove the outliers in the data using mahalanobis distance. 
attach(master_data)
MD <- mahalanobis(master_data[,c(numericAtt)], colMeans(master_data[,c(numericAtt)]),cov(master_data[,c(numericAtt)]),tol=1e-20)
master_data$MD <- round(MD,3)
master_data$Outlier_Mahalanobis <- "No"
master_data$Outlier_Mahalanobis[master_data$MD > 12] <- "Yes" #Threshold i did chose was 9. 
master_data <- master_data[master_data$Outlier_Mahalanobis == "No",]
master_data$Outlier_Mahalanobis <- NULL
master_data$MD <- NULL #We can remove these now. 
```


```{r}
#visualizing correlations. 
boxplot(master_data[numericAtt], xlab = "Numeric Attributes", ylab = "Count", main = "Boxplots of All Numeric Attributes")
grid(20,20, col = "lightgray", lty = "dotted",lwd = par("lwd"), equilogs = TRUE) 
#population has outliers does it matter 
Correlations <- cor(master_data[,numericAtt])
corrplot(Correlations) 
#We see there are a few highly correlated  independent variables. We will use feature selection. 
```


```{r}
#Dealing with imbalance 
table(master_data$Status)
Balanced_Data <- ovun.sample(Status ~ ., data = master_data, method = "both",p = 0.5)$data #Utilizes both over and under sampling. ** DEBUG ROSE**
table(Balanced_Data$Status)
```


```{r}
#Test for nomality
lapply(Balanced_Data[,numericAtt], ad.test) #Anderson Darling Test
lapply(Balanced_Data[1:5000,numericAtt], shapiro.test) # Shapiro-Wilk Test 
#Both tests have p values under 0.05. Therefore, we can reject the null hypothesis of normality. 
```


```{r}
#now that we have processed the data, removed worthy outliers, dealth with imbalanace, tested for normality and saw visually what our data is like, 
#we can go ahead and perform feature selection and extraction via information 

#Feature Selection via information gain
weights <- information.gain(Status~., master_data[,numericAtt])
print(weights)
subset <- cutoff.k(weights, 4) #mean is 0.13
f <- as.simple.formula(subset, "Status")
print(f)
#Info gain gave us four features. 

#Variance Inflation Factor
vif(master_data[,numericAtt])

```


```{r}

#PCA - Since we cannot do PCA on the entire dataset at once (due to exposing the test set), we will use a 70/30 split and perform PCA on the training set. Once we obtain our principal components, we will run a logisitc regression with it in the next step. 
ScaledData <- as.data.frame(scale(Balanced_Data[,numericAtt]))
smp_size <- floor(0.75 * nrow(ScaledData))
train_ind <- sample(seq_len(nrow(ScaledData)), size = smp_size)
TRAINPCA <- ScaledData[train_ind, ]
TESTPCA <- ScaledData[-train_ind, ]
PCATRAIN <- prcomp(TRAINPCA, scale = FALSE, center= FALSE)
LoadingMatrix <- PCATRAIN$rotation
dim(PCATRAIN$x) #has principal component score vectors in a 8336 x 12 matrix.
biplot(PCATRAIN, scale = 0)
PCAstd_dev <- PCATRAIN$sdev
PCA_var <- PCAstd_dev^2
#proportion of variance explained
varianceexplained <- PCA_var/sum(PCA_var)
plot(varianceexplained, type = "b", main = 'Components With Standardization')
plot(cumsum(varianceexplained), xlab = "Components", ylab = "Variance Explained", main = "Variance Explained with each Component", col = "red")
#We see that the first 8 principal components explained around 95% of the variance in the data. We will go with that. 
```


```{r}

#Elementary analysis 
#Here, we see based on our information gain what the characteristics of our successful projects are in regards to our varibales
#defined by the info gain. 
sucbacker <- sqldf::sqldf("SELECT Backers FROM master_Data WHERE Status = 'successful'")
mean(sucbacker$Backers)
sucup<- sqldf::sqldf("SELECT Updates FROM Balanced_Data WHERE Status = 'successful'")
mean(sucup$Updates)
succomments <- sqldf::sqldf("SELECT Comments FROM master_data WHERE Status = 'successful'")
mean(succomments$Comments)
sucPledged <- sqldf::sqldf("SELECT Pledged FROM master_data WHERE Status = 'successful'")
mean(sucPledged$Pledged)
nrow(succomments)/nrow(master_data)
```


```{r}
#Now that we have done PCA and information gain, we will test the logisitc regression agains the features determined by the information gain as well as the principal components. We will use each.  
#Created a new dataset for the logisitc regression. I created a loop to test the model under different amounts of trianing data folds. 
Balanced_DataLOG <- Balanced_Data %>% mutate_if(is.numeric, scale) #Scaling Balanced Data Set
Balanced_DataLOG<-Balanced_DataLOG[sample(nrow(Balanced_DataLOG)),]
folds <- cut(seq(1,nrow(Balanced_DataLOG)),breaks=30,labels=FALSE)
AUCValue <- vector("numeric",10L)
set.seed(500)
#Perform 10 fold cross validation on our first model: Logistic Regression
for(i in 1:30){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  TESTLOGI <- Balanced_DataLOG[testIndexes, ]
  TRAINLOGI <- Balanced_DataLOG[-testIndexes, ]
  LogisticMod <- glm(Status ~  Goal + Updates + Duration + Pledge_per_person + Population, family = "binomial", data = TRAINLOGI)
  pred <- predict(LogisticMod,TESTLOGI, type = "response")
  RocVal <- roc(TESTLOGI$Status,pred)
  AUCValue[i] <- auc(RocVal)
}
AUCValue
KFolds <- c(1:30)
scatter.smooth(KFolds,AUCValue, col = c("Blue","red"), main = "Folds Vs AUC Values") #We see the same pattern. I say about 25 folds is optimal.v AUC = 0.9529070

fold.size <- nrow(Balanced_DataLOG) / 30
TRAINLOGI <- Balanced_DataLOG[1:(25*fold.size),]
TESTLOGI <- Balanced_DataLOG[(26*fold.size):(30*fold.size),]
LogisticMod <- glm(Status ~  Pledged + Backers + Updates + Comments, family = "binomial", data = TRAINLOGI)
pred <- predict(LogisticMod,TESTLOGI, type = "response")
RocVal <- roc(TESTLOGI$Status,pred)
RocVal
```


```{r}

#We will now perform the same method but with 8 principal components
#we are interested in first 8 PCs
set.seed(500)
TRAINPCA <- merge(TRAINPCA,Balanced_Data$Status, by = 0)
colnames(TRAINPCA)[12] <- "Status"
TRAINPCA[1] = NULL
train.data.PCA <- data.frame(Status = TRAINPCA$Status, PCATRAIN$x)
train.data.PCA <- train.data.PCA[,1:9]
ModLOGPCA <- glm(Status~., family = "binomial", data = train.data.PCA)
PCATest <- prcomp(TESTPCA, scale = FALSE, center= FALSE)
TESTPCA <- merge(TESTPCA,Balanced_Data$Status, by = 0)
colnames(TESTPCA)[12] <- "Status"
TESTPCA[1] = NULL
test.data.PCA <- data.frame(Status = TESTPCA$Status, PCATest$x)
test.data.PCA <- test.data.PCA[,1:9]
prediction <- predict(ModLOGPCA , test.data.PCA)
RocVal <- roc(test.data.PCA$Status,prediction)
AUCValue<- auc(RocVal)
AUCValue

#We see that PCA doesnt perform as well as the cross fold. 


#Logistic Regression using optimal Percentage Split
set.seed(500)
smp_size <- floor(0.75 * nrow(Balanced_Data))
train_ind <- sample(seq_len(nrow(Balanced_DataLOG)), size = smp_size)
train <- Balanced_DataLOG[train_ind, ]
test <- Balanced_DataLOG[-train_ind, ]
ModLOGPERCSPLIT <- glm(Status ~ Pledged + Backers + Updates + Comments + Pledge_per_person, family = "binomial", data = train)
predPercSplit <- predict(ModLOGPERCSPLIT ,test, type = "response")
auc(roc(test$Status,predPercSplit)) #0.957 roc and auc give different auc vals?

```


```{r}
#KNN
#___________________________________________________________________________
#Standardize
Balanced_DataKNN <- Balanced_Data %>% mutate_if(is.numeric, scale)
Balanced_DataKNN <- Balanced_DataKNN[,c(4,numericAtt)]
Balanced_DataKNN$Status <- as.numeric(Balanced_DataKNN$Status) #converted successful and failed to 1 and 2
Balanced_DataKNN <- Balanced_DataKNN[sample(1:nrow(Balanced_Data)), ]
set.seed(500)
for (i in seq(.1,1,.20)) {
  index = createDataPartition(Balanced_DataKNN$Status, p = i, list = F )
  KNNtrain = Balanced_DataKNN[index,]
  KNNtest = Balanced_DataKNN[-index,]
  Balanced_Train_labels <- Balanced_DataKNN[1:nrow(KNNtrain),1]
  Balanced_Test_labels <- Balanced_DataKNN[(nrow(KNNtrain)+1):nrow(Balanced_DataKNN),1]
  
  KnnPredictions <- knn(train = KNNtrain, test = KNNtest,cl = Balanced_Train_labels, k=10)
  CT <- CrossTable(x=Balanced_Test_labels, y=KnnPredictions, prop.chisq=FALSE)
  print(sum(diag(CT$tab))/sum(CT$tab))
  
}
```


```{r}

#Using Support Vector Machines. using 75/25 split
set.seed(500)
SVMData <- Balanced_Data %>% mutate_if(is.numeric, scale)
SVMData <- SVMData[,c(4,numericAtt)]
smp_size <- floor(0.70 * nrow(SVMData))
train_ind <- sample(seq_len(nrow(SVMData)), size = smp_size)
SVMTrain <- SVMData[train_ind,]
SVMTest <- SVMData[-train_ind,]
svm.model <- svm(Status ~., data = SVMTrain, cost = 100, gamma = 1)
svm.pred <- predict(svm.model,SVMTest)
table(svm.pred,SVMTest$Status)
confusionMatrix(svm.pred,SVMTest$Status)
```


```{r}
#Using random Forest. We do not need to scale as Random forest involved Tree partitioning algorithms
BalancedRF <- Balanced_Data
BalancedRF <- BalancedRF[,-c(1,2,11,12,18,20,21)]
BalancedRF$Usd_Pledged <- as.numeric(BalancedRF$Usd_Pledged)
train_ind <- sample(nrow(BalancedRF), 0.7*nrow(BalancedRF), replace = FALSE)
RFTrain <- BalancedRF[train_ind,]
RFTest <- BalancedRF[-train_ind,]
RFTest <- RFTest[sample(1:nrow(RFTest)),]
Accuracy = vector("numeric",10L)
for (i in seq(1,13,1)) {
  RFModel <-randomForest(Status~ ., data = RFTrain, importance = TRUE, mtry = i, ntree = 300) 
  Pred <- predict(RFModel,RFTest)
  Accuracy[i] = mean(Pred == RFTest$Status)
}

abline(plot(1:13,Accuracy, xlab = "MTry", ylab = "Mean of Correct Predictions", main = "Random Forest MTry vs Accuracy"))
#Starts to platow at mtry = 7. Those are how many variables we want to use to inspect at every split. 

RFModel <-randomForest(Status~ ., data = RFTrain, importance = TRUE, mtry = 7, ntree = 300) 
Pred <- predict(RFModel,RFTest)
mean(Pred == RFTest$Status)
```


```{r}
#Ensemble Technique  
nrow(TRAINLOGI)
nrow(KNNtrain)
nrow(SVMTrain)
nrow(RFTrain)
sum <- 7250+7830+6090+6090
sum/4
#since each of our models have different traiing sets and i believe thqt to train all our models on the same training set defeats the purpose of my work in trying
#to find optimal folds and splits, i have averaged the amount of rows in each training set and will use that number as the training set for our ensemble.

control <- trainControl(sampling="rose",method="repeatedcv", number=5, repeats=2, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c( 'knn','glm','ranger','svmLinear')
set.seed(1234)
Balanced_Data <- Balanced_Data[,-c(1,2,3,11,12,18)] #We decide to remove these columns because they are dates and factor types. some algos only handle numeric
Samplesize <- floor(0.783*nrow(Balanced_Data))
train_ind <- sample(seq_len(nrow(Balanced_DataLOG)), size = Samplesize)
EnsembleTrain <- Balanced_Data[train_ind,]
EnsembleTest <- Balanced_Data[-train_ind,]
EnsembleTest <- EnsembleTest[sample(nrow(EnsembleTest)),]
stack_models <- caretList(Status~., data=EnsembleTrain, trControl=control, methodList=algorithmList)
stacking_results <- resamples(stack_models)
summary(stacking_results)
dotplot(stacking_results)
modelCor(stacking_results)
splom(stacking_results)

metric <- "Accuracy"
# stack using Logistics Regression
stackControl <- trainControl(sampling="rose",method="repeatedcv", number=5, repeats=2, savePredictions=TRUE, classProbs=TRUE)
stack.glm <- caretStack(stack_models, method="glm", metric=metric, trControl=stackControl)
print(stack.glm) #high overall accuracy and not bad kappa. Kappa tells us how well the classification performed as compared to just randomly assigning values
# evaluate results on test set
EnsembleTest$pred <- predict(stack.glm, newdata=EnsembleTest)
confusionMatrix(data = EnsembleTest$pred, reference = EnsembleTest$Status)
```








